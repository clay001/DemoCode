{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def hop_getGuo():\n",
    "    #布尔数据\n",
    "    #bool_data = pd.read_csv(\"input_data/bool438.csv\")   \n",
    "    #距离矩阵\n",
    "    #dist_matrix = pd.read_csv(\"input_data/guo_distanceMatrix.csv\")    \n",
    "    #能量矩阵\n",
    "    #energy_matrix = pd.read_csv(\"input_data/guo_energyMatrix.csv\")\n",
    "    \n",
    "    #原始数据 从95开始是16C\n",
    "    data = pd.read_csv(\"input_data/guo438.csv\")\n",
    "    \n",
    "    #权重矩阵\n",
    "    guo_weight = pd.read_csv(\"input_data/guo_weightMatrix.csv\")\n",
    "    guo_weight = guo_weight.values[:,1:]\n",
    "    guo_weight = guo_weight.astype('float64')\n",
    "    \n",
    "    \n",
    "    # 取出16-64的细胞及其在48个基因上的表达矩阵\n",
    "    data = data.values[:,1:]\n",
    "    data = data.astype('float64')\n",
    "    lis = [8,28,38,58,108,178,428]\n",
    "    data = data[lis]\n",
    "    # 8,28,38,58,108,178,428\n",
    "    \n",
    "    \n",
    "    #guo_weight.index = guo_weight.columns[1:]\n",
    "    #weightGuo = guo_weight.loc[guo2names][guo2names].values\n",
    "    \n",
    "    return data,guo_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import GetData\n",
    "import pandas as pd\n",
    "import validation as val\n",
    "\n",
    "#(exp_data,init_weight,energy_data) = GetData.getSco2() \n",
    "exp_data,init_weight = hop_getGuo() \n",
    "#get GUO_data: GetData.getGuo()\n",
    "(numGene, numData)= (np.shape(exp_data)[1],np.shape(exp_data)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "###############  Define a cell in RNN ###########\n",
    "class ODECell(Module):\n",
    "    def __init__(self, n_inputs, n_neurons,h):\n",
    "        super(ODECell, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.h = h\n",
    "        \n",
    "        ### self.Wx: Weight is a gene network, represented by a matrix,大小为input,neurons\n",
    "        # \n",
    "        self.Wx = Parameter(torch.Tensor(n_inputs, n_neurons))\n",
    "        \n",
    "        ### self.diag:A diagnal matrix; \n",
    "        self.diag = Parameter(torch.Tensor(n_inputs))\n",
    "        self.bias = Parameter(torch.Tensor(1,n_inputs))\n",
    "        self.const =  Parameter(torch.Tensor(1,n_inputs))\n",
    "        # 自定义的函数，用于初始化\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    #之前是初始化的问题\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.n_neurons)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-0.001, 0.001)\n",
    "            \n",
    "            \n",
    "    def forward(self,X0):\n",
    "        ###### Define ODE: ######\n",
    "        Y0 = torch.mm(torch.tanh(X0), self.Wx)\n",
    "        Y0 = Y0.mul(self.const)\n",
    "        Y0 = Y0 - torch.mm(X0,torch.diag(self.diag)) + self.bias\n",
    "        #Y0 = torch.tanh(torch.mm(X0, self.Wx)-torch.mm(X0,torch.diag(self.diag)))\n",
    "        ### \n",
    "        Y1 = X0 +  self.h * Y0\n",
    "        #Y1 = torch.tanh(Y1)\n",
    "        return Y1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Define RNN ###################\n",
    "import math\n",
    "class ODERNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons,h):\n",
    "        super(ODERNN, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.h = h\n",
    "        ### self.h: time interval in numerial ODE\n",
    "        self.cell = ODECell(n_inputs, n_neurons,h)\n",
    "        \n",
    "        \n",
    "    def forward(self, X0,steps):\n",
    "        out = torch.empty(steps,self.n_inputs)\n",
    "        nn.init.zeros_(out)\n",
    "        out[0] = X0\n",
    "        \n",
    "        for i in range(steps-1):\n",
    "            X0 = self.cell(X0)\n",
    "            out[i+1] = X0\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "#### initialize parameters #####\n",
    "#odernn.cell.Wx = Parameter(torch.from_numpy(init_weight).float())\n",
    "#odernn.cell.diag = Parameter(torch.from_numpy(np.full((numGene),6)).float())\n",
    "odernn = ODERNN(numGene,numGene,1)\n",
    "\n",
    "EPOCH = 200000\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(odernn.parameters(),lr=0.001)\n",
    "\n",
    "inputs = torch.tensor([exp_data[0]],dtype = torch.float)\n",
    "datas = Variable(torch.from_numpy(exp_data))\n",
    "datas = datas.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.5582e-04, -9.2494e-04, -7.4032e-04,  ...,  5.8811e-04,\n",
       "          4.2603e-04,  7.3910e-04],\n",
       "        [ 9.1378e-04,  2.7320e-04,  1.9428e-04,  ..., -3.2818e-04,\n",
       "         -2.3310e-04,  6.7948e-04],\n",
       "        [ 5.2648e-05, -4.7650e-04, -1.8114e-04,  ..., -5.3971e-04,\n",
       "          5.3513e-04,  7.8762e-04],\n",
       "        ...,\n",
       "        [-9.6928e-04, -5.1259e-04, -4.2113e-04,  ..., -4.1423e-04,\n",
       "          8.0627e-04, -1.0572e-04],\n",
       "        [ 9.7814e-04,  3.1331e-04, -1.3104e-04,  ..., -5.5724e-04,\n",
       "          5.9849e-05,  4.1771e-04],\n",
       "        [-4.5738e-04,  5.4247e-04, -8.0256e-04,  ..., -4.9244e-04,\n",
       "         -5.8244e-05, -6.3667e-04]], requires_grad=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odernn.cell.Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 20.9751\n",
      "Epoch:  500 | train loss: 7.5602\n",
      "Epoch:  1000 | train loss: 7.2374\n",
      "Epoch:  1500 | train loss: 6.6775\n",
      "Epoch:  2000 | train loss: 6.0202\n",
      "Epoch:  2500 | train loss: 5.4320\n",
      "Epoch:  3000 | train loss: 4.9875\n",
      "Epoch:  3500 | train loss: 4.6732\n",
      "Epoch:  4000 | train loss: 4.4252\n",
      "Epoch:  4500 | train loss: 4.2837\n",
      "Epoch:  5000 | train loss: 4.2221\n",
      "Epoch:  5500 | train loss: 4.1993\n",
      "Epoch:  6000 | train loss: 4.1908\n",
      "Epoch:  6500 | train loss: 4.1863\n",
      "Epoch:  7000 | train loss: 4.1841\n",
      "Epoch:  7500 | train loss: 4.1807\n",
      "Epoch:  8000 | train loss: 4.1787\n",
      "Epoch:  8500 | train loss: 4.1771\n",
      "Epoch:  9000 | train loss: 4.1753\n",
      "Epoch:  9500 | train loss: 4.1744\n",
      "Epoch:  10000 | train loss: 4.1730\n",
      "Epoch:  10500 | train loss: 4.1713\n",
      "Epoch:  11000 | train loss: 4.1705\n",
      "Epoch:  11500 | train loss: 4.1692\n",
      "Epoch:  12000 | train loss: 4.1681\n",
      "Epoch:  12500 | train loss: 4.1673\n",
      "Epoch:  13000 | train loss: 4.1668\n",
      "Epoch:  13500 | train loss: 4.1659\n",
      "Epoch:  14000 | train loss: 4.1648\n",
      "Epoch:  14500 | train loss: 4.1646\n",
      "Epoch:  15000 | train loss: 4.1634\n",
      "Epoch:  15500 | train loss: 4.1627\n",
      "Epoch:  16000 | train loss: 4.1637\n",
      "Epoch:  16500 | train loss: 4.1614\n",
      "Epoch:  17000 | train loss: 4.1612\n",
      "Epoch:  17500 | train loss: 4.1605\n",
      "Epoch:  18000 | train loss: 4.1599\n",
      "Epoch:  18500 | train loss: 4.1594\n",
      "Epoch:  19000 | train loss: 4.1592\n",
      "Epoch:  19500 | train loss: 4.1590\n",
      "Epoch:  20000 | train loss: 4.1589\n",
      "Epoch:  20500 | train loss: 4.1575\n",
      "Epoch:  21000 | train loss: 4.1572\n",
      "Epoch:  21500 | train loss: 4.1568\n",
      "Epoch:  22000 | train loss: 4.1564\n",
      "Epoch:  22500 | train loss: 4.1558\n",
      "Epoch:  23000 | train loss: 4.1558\n",
      "Epoch:  23500 | train loss: 4.1552\n",
      "Epoch:  24000 | train loss: 4.1548\n",
      "Epoch:  24500 | train loss: 4.1544\n",
      "Epoch:  25000 | train loss: 4.1542\n",
      "Epoch:  25500 | train loss: 4.1538\n",
      "Epoch:  26000 | train loss: 4.1536\n",
      "Epoch:  26500 | train loss: 4.1564\n",
      "Epoch:  27000 | train loss: 4.1532\n",
      "Epoch:  27500 | train loss: 4.1528\n",
      "Epoch:  28000 | train loss: 4.1528\n",
      "Epoch:  28500 | train loss: 4.1523\n",
      "Epoch:  29000 | train loss: 4.1527\n",
      "Epoch:  29500 | train loss: 4.1520\n",
      "Epoch:  30000 | train loss: 4.1513\n",
      "Epoch:  30500 | train loss: 4.1512\n",
      "Epoch:  31000 | train loss: 4.1514\n",
      "Epoch:  31500 | train loss: 4.1511\n",
      "Epoch:  32000 | train loss: 4.1504\n",
      "Epoch:  32500 | train loss: 4.1510\n",
      "Epoch:  33000 | train loss: 4.1498\n",
      "Epoch:  33500 | train loss: 4.1500\n",
      "Epoch:  34000 | train loss: 4.1497\n",
      "Epoch:  34500 | train loss: 4.1505\n",
      "Epoch:  35000 | train loss: 4.1489\n",
      "Epoch:  35500 | train loss: 4.1491\n",
      "Epoch:  36000 | train loss: 4.1497\n",
      "Epoch:  36500 | train loss: 4.1485\n",
      "Epoch:  37000 | train loss: 4.1491\n",
      "Epoch:  37500 | train loss: 4.1480\n",
      "Epoch:  38000 | train loss: 4.1477\n",
      "Epoch:  38500 | train loss: 4.1478\n",
      "Epoch:  39000 | train loss: 4.1473\n",
      "Epoch:  39500 | train loss: 4.1476\n",
      "Epoch:  40000 | train loss: 4.1472\n",
      "Epoch:  40500 | train loss: 4.1470\n",
      "Epoch:  41000 | train loss: 4.1487\n",
      "Epoch:  41500 | train loss: 4.1469\n",
      "Epoch:  42000 | train loss: 4.1464\n",
      "Epoch:  42500 | train loss: 4.1511\n",
      "Epoch:  43000 | train loss: 4.1460\n",
      "Epoch:  43500 | train loss: 4.1460\n",
      "Epoch:  44000 | train loss: 4.1462\n",
      "Epoch:  44500 | train loss: 4.1469\n",
      "Epoch:  45000 | train loss: 4.1457\n",
      "Epoch:  45500 | train loss: 4.1452\n",
      "Epoch:  46000 | train loss: 4.1463\n",
      "Epoch:  46500 | train loss: 4.1453\n",
      "Epoch:  47000 | train loss: 4.1449\n",
      "Epoch:  47500 | train loss: 4.1447\n",
      "Epoch:  48000 | train loss: 4.1446\n",
      "Epoch:  48500 | train loss: 4.1448\n",
      "Epoch:  49000 | train loss: 4.1443\n",
      "Epoch:  49500 | train loss: 4.1450\n",
      "Epoch:  50000 | train loss: 4.1441\n",
      "Epoch:  50500 | train loss: 4.1443\n",
      "Epoch:  51000 | train loss: 4.1437\n",
      "Epoch:  51500 | train loss: 4.1439\n",
      "Epoch:  52000 | train loss: 4.1436\n",
      "Epoch:  52500 | train loss: 4.1436\n",
      "Epoch:  53000 | train loss: 4.1433\n",
      "Epoch:  53500 | train loss: 4.1434\n",
      "Epoch:  54000 | train loss: 4.1431\n",
      "Epoch:  54500 | train loss: 4.1451\n",
      "Epoch:  55000 | train loss: 4.1437\n",
      "Epoch:  55500 | train loss: 4.1435\n",
      "Epoch:  56000 | train loss: 4.1433\n",
      "Epoch:  56500 | train loss: 4.1441\n",
      "Epoch:  57000 | train loss: 4.1489\n",
      "Epoch:  57500 | train loss: 4.1423\n",
      "Epoch:  58000 | train loss: 4.1429\n",
      "Epoch:  58500 | train loss: 4.1421\n",
      "Epoch:  59000 | train loss: 4.1420\n",
      "Epoch:  59500 | train loss: 4.1422\n",
      "Epoch:  60000 | train loss: 4.1422\n",
      "Epoch:  60500 | train loss: 4.1456\n",
      "Epoch:  61000 | train loss: 4.1440\n",
      "Epoch:  61500 | train loss: 4.1420\n",
      "Epoch:  62000 | train loss: 4.1427\n",
      "Epoch:  62500 | train loss: 4.1413\n",
      "Epoch:  63000 | train loss: 4.1413\n",
      "Epoch:  63500 | train loss: 4.1419\n",
      "Epoch:  64000 | train loss: 4.1455\n",
      "Epoch:  64500 | train loss: 4.1410\n",
      "Epoch:  65000 | train loss: 4.1418\n",
      "Epoch:  65500 | train loss: 4.1407\n",
      "Epoch:  66000 | train loss: 4.1422\n",
      "Epoch:  66500 | train loss: 4.1418\n",
      "Epoch:  67000 | train loss: 4.1411\n",
      "Epoch:  67500 | train loss: 4.1405\n",
      "Epoch:  68000 | train loss: 4.1433\n",
      "Epoch:  68500 | train loss: 4.1406\n",
      "Epoch:  69000 | train loss: 4.1404\n",
      "Epoch:  69500 | train loss: 4.1405\n",
      "Epoch:  70000 | train loss: 4.1511\n",
      "Epoch:  70500 | train loss: 4.1403\n",
      "Epoch:  71000 | train loss: 4.1406\n",
      "Epoch:  71500 | train loss: 4.1399\n",
      "Epoch:  72000 | train loss: 4.1397\n",
      "Epoch:  72500 | train loss: 4.1407\n",
      "Epoch:  73000 | train loss: 4.1395\n",
      "Epoch:  73500 | train loss: 4.1412\n",
      "Epoch:  74000 | train loss: 4.1400\n",
      "Epoch:  74500 | train loss: 4.1404\n",
      "Epoch:  75000 | train loss: 4.1488\n",
      "Epoch:  75500 | train loss: 4.1399\n",
      "Epoch:  76000 | train loss: 4.1402\n",
      "Epoch:  76500 | train loss: 4.1390\n",
      "Epoch:  77000 | train loss: 4.1388\n",
      "Epoch:  77500 | train loss: 4.1390\n",
      "Epoch:  78000 | train loss: 4.1389\n",
      "Epoch:  78500 | train loss: 4.1386\n",
      "Epoch:  79000 | train loss: 4.1388\n",
      "Epoch:  79500 | train loss: 4.1398\n",
      "Epoch:  80000 | train loss: 4.1388\n",
      "Epoch:  80500 | train loss: 4.1383\n",
      "Epoch:  81000 | train loss: 4.1394\n",
      "Epoch:  81500 | train loss: 4.1384\n",
      "Epoch:  82000 | train loss: 4.1389\n",
      "Epoch:  82500 | train loss: 4.1474\n",
      "Epoch:  83000 | train loss: 4.1391\n",
      "Epoch:  83500 | train loss: 4.1380\n",
      "Epoch:  84000 | train loss: 4.1379\n",
      "Epoch:  84500 | train loss: 4.1495\n",
      "Epoch:  85000 | train loss: 4.1377\n",
      "Epoch:  85500 | train loss: 4.1377\n",
      "Epoch:  86000 | train loss: 4.1402\n",
      "Epoch:  86500 | train loss: 4.1376\n",
      "Epoch:  87000 | train loss: 4.1452\n",
      "Epoch:  87500 | train loss: 4.1396\n",
      "Epoch:  88000 | train loss: 4.1376\n",
      "Epoch:  88500 | train loss: 4.1373\n",
      "Epoch:  89000 | train loss: 4.1377\n",
      "Epoch:  89500 | train loss: 4.1374\n",
      "Epoch:  90000 | train loss: 4.1380\n",
      "Epoch:  90500 | train loss: 4.1386\n",
      "Epoch:  91000 | train loss: 4.1378\n",
      "Epoch:  91500 | train loss: 4.1376\n",
      "Epoch:  92000 | train loss: 4.1418\n",
      "Epoch:  92500 | train loss: 4.1377\n",
      "Epoch:  93000 | train loss: 4.1371\n",
      "Epoch:  93500 | train loss: 4.1368\n",
      "Epoch:  94000 | train loss: 4.1389\n",
      "Epoch:  94500 | train loss: 4.1367\n",
      "Epoch:  95000 | train loss: 4.1374\n",
      "Epoch:  95500 | train loss: 4.1407\n",
      "Epoch:  96000 | train loss: 4.1365\n",
      "Epoch:  96500 | train loss: 4.1365\n",
      "Epoch:  97000 | train loss: 4.1363\n",
      "Epoch:  97500 | train loss: 4.1366\n",
      "Epoch:  98000 | train loss: 4.1364\n",
      "Epoch:  98500 | train loss: 4.1367\n",
      "Epoch:  99000 | train loss: 4.1370\n",
      "Epoch:  99500 | train loss: 4.1362\n",
      "Epoch:  100000 | train loss: 4.1361\n",
      "Epoch:  100500 | train loss: 4.1361\n",
      "Epoch:  101000 | train loss: 4.1364\n",
      "Epoch:  101500 | train loss: 4.1359\n",
      "Epoch:  102000 | train loss: 4.1359\n",
      "Epoch:  102500 | train loss: 4.1357\n",
      "Epoch:  103000 | train loss: 4.1362\n",
      "Epoch:  103500 | train loss: 4.1368\n",
      "Epoch:  104000 | train loss: 4.1377\n",
      "Epoch:  104500 | train loss: 4.1368\n",
      "Epoch:  105000 | train loss: 4.1363\n",
      "Epoch:  105500 | train loss: 4.1357\n",
      "Epoch:  106000 | train loss: 4.1355\n",
      "Epoch:  106500 | train loss: 4.1421\n",
      "Epoch:  107000 | train loss: 4.1361\n",
      "Epoch:  107500 | train loss: 4.1352\n",
      "Epoch:  108000 | train loss: 4.1356\n",
      "Epoch:  108500 | train loss: 4.1376\n",
      "Epoch:  109000 | train loss: 4.1386\n",
      "Epoch:  109500 | train loss: 4.1362\n",
      "Epoch:  110000 | train loss: 4.1353\n",
      "Epoch:  110500 | train loss: 4.1349\n",
      "Epoch:  111000 | train loss: 4.1371\n",
      "Epoch:  111500 | train loss: 4.1349\n",
      "Epoch:  112000 | train loss: 4.1354\n",
      "Epoch:  112500 | train loss: 4.1360\n",
      "Epoch:  113000 | train loss: 4.1347\n",
      "Epoch:  113500 | train loss: 4.1362\n",
      "Epoch:  114000 | train loss: 4.1373\n",
      "Epoch:  114500 | train loss: 4.1345\n",
      "Epoch:  115000 | train loss: 4.1426\n",
      "Epoch:  115500 | train loss: 4.1348\n",
      "Epoch:  116000 | train loss: 4.1357\n",
      "Epoch:  116500 | train loss: 4.1345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  117000 | train loss: 4.1343\n",
      "Epoch:  117500 | train loss: 4.1344\n",
      "Epoch:  118000 | train loss: 4.1342\n",
      "Epoch:  118500 | train loss: 4.1344\n",
      "Epoch:  119000 | train loss: 4.1351\n",
      "Epoch:  119500 | train loss: 4.1341\n",
      "Epoch:  120000 | train loss: 4.1342\n",
      "Epoch:  120500 | train loss: 4.1347\n",
      "Epoch:  121000 | train loss: 4.1368\n",
      "Epoch:  121500 | train loss: 4.1381\n",
      "Epoch:  122000 | train loss: 4.1347\n",
      "Epoch:  122500 | train loss: 4.1434\n",
      "Epoch:  123000 | train loss: 4.1352\n",
      "Epoch:  123500 | train loss: 4.1346\n",
      "Epoch:  124000 | train loss: 4.1339\n",
      "Epoch:  124500 | train loss: 4.1433\n",
      "Epoch:  125000 | train loss: 4.1347\n",
      "Epoch:  125500 | train loss: 4.1341\n",
      "Epoch:  126000 | train loss: 4.1386\n",
      "Epoch:  126500 | train loss: 4.1423\n",
      "Epoch:  127000 | train loss: 4.1364\n",
      "Epoch:  127500 | train loss: 4.1341\n",
      "Epoch:  128000 | train loss: 4.1352\n",
      "Epoch:  128500 | train loss: 4.1411\n",
      "Epoch:  129000 | train loss: 4.1372\n",
      "Epoch:  129500 | train loss: 4.1373\n",
      "Epoch:  130000 | train loss: 4.1351\n",
      "Epoch:  130500 | train loss: 4.1339\n",
      "Epoch:  131000 | train loss: 4.1336\n",
      "Epoch:  131500 | train loss: 4.1365\n",
      "Epoch:  132000 | train loss: 4.1332\n",
      "Epoch:  132500 | train loss: 4.1331\n",
      "Epoch:  133000 | train loss: 4.1331\n",
      "Epoch:  133500 | train loss: 4.1335\n",
      "Epoch:  134000 | train loss: 4.1358\n",
      "Epoch:  134500 | train loss: 4.1331\n",
      "Epoch:  135000 | train loss: 4.1357\n",
      "Epoch:  135500 | train loss: 4.1331\n",
      "Epoch:  136000 | train loss: 4.1369\n",
      "Epoch:  136500 | train loss: 4.1390\n",
      "Epoch:  137000 | train loss: 4.1351\n",
      "Epoch:  137500 | train loss: 4.1328\n",
      "Epoch:  138000 | train loss: 4.1342\n",
      "Epoch:  138500 | train loss: 4.1327\n",
      "Epoch:  139000 | train loss: 4.1334\n",
      "Epoch:  139500 | train loss: 4.1327\n",
      "Epoch:  140000 | train loss: 4.1336\n",
      "Epoch:  140500 | train loss: 4.1327\n",
      "Epoch:  141000 | train loss: 4.1342\n",
      "Epoch:  141500 | train loss: 4.1348\n",
      "Epoch:  142000 | train loss: 4.1337\n",
      "Epoch:  142500 | train loss: 4.1356\n",
      "Epoch:  143000 | train loss: 4.1330\n",
      "Epoch:  143500 | train loss: 4.1337\n",
      "Epoch:  144000 | train loss: 4.1324\n",
      "Epoch:  144500 | train loss: 4.1345\n",
      "Epoch:  145000 | train loss: 4.1340\n",
      "Epoch:  145500 | train loss: 4.1579\n",
      "Epoch:  146000 | train loss: 4.1324\n",
      "Epoch:  146500 | train loss: 4.1328\n",
      "Epoch:  147000 | train loss: 4.1332\n",
      "Epoch:  147500 | train loss: 4.1336\n",
      "Epoch:  148000 | train loss: 4.1321\n",
      "Epoch:  148500 | train loss: 4.1339\n",
      "Epoch:  149000 | train loss: 4.1321\n",
      "Epoch:  149500 | train loss: 4.1352\n",
      "Epoch:  150000 | train loss: 4.1331\n",
      "Epoch:  150500 | train loss: 4.1335\n",
      "Epoch:  151000 | train loss: 4.1323\n",
      "Epoch:  151500 | train loss: 4.1318\n",
      "Epoch:  152000 | train loss: 4.1318\n",
      "Epoch:  152500 | train loss: 4.1320\n",
      "Epoch:  153000 | train loss: 4.1449\n",
      "Epoch:  153500 | train loss: 4.1326\n",
      "Epoch:  154000 | train loss: 4.1319\n",
      "Epoch:  154500 | train loss: 4.1317\n",
      "Epoch:  155000 | train loss: 4.1316\n",
      "Epoch:  155500 | train loss: 4.1336\n",
      "Epoch:  156000 | train loss: 4.1482\n",
      "Epoch:  156500 | train loss: 4.1328\n",
      "Epoch:  157000 | train loss: 4.1373\n",
      "Epoch:  157500 | train loss: 4.1317\n",
      "Epoch:  158000 | train loss: 4.1316\n",
      "Epoch:  158500 | train loss: 4.1329\n",
      "Epoch:  159000 | train loss: 4.1313\n",
      "Epoch:  159500 | train loss: 4.1326\n",
      "Epoch:  160000 | train loss: 4.1318\n",
      "Epoch:  160500 | train loss: 4.1344\n",
      "Epoch:  161000 | train loss: 4.1334\n",
      "Epoch:  161500 | train loss: 4.1312\n",
      "Epoch:  162000 | train loss: 4.1321\n",
      "Epoch:  162500 | train loss: 4.1314\n",
      "Epoch:  163000 | train loss: 4.1396\n",
      "Epoch:  163500 | train loss: 4.1317\n",
      "Epoch:  164000 | train loss: 4.1311\n",
      "Epoch:  164500 | train loss: 4.1310\n",
      "Epoch:  165000 | train loss: 4.1336\n",
      "Epoch:  165500 | train loss: 4.1317\n",
      "Epoch:  166000 | train loss: 4.1323\n",
      "Epoch:  166500 | train loss: 4.1328\n",
      "Epoch:  167000 | train loss: 4.1310\n",
      "Epoch:  167500 | train loss: 4.1312\n",
      "Epoch:  168000 | train loss: 4.1313\n",
      "Epoch:  168500 | train loss: 4.1327\n",
      "Epoch:  169000 | train loss: 4.1308\n",
      "Epoch:  169500 | train loss: 4.1309\n",
      "Epoch:  170000 | train loss: 4.1338\n",
      "Epoch:  170500 | train loss: 4.1309\n",
      "Epoch:  171000 | train loss: 4.1307\n",
      "Epoch:  171500 | train loss: 4.1316\n",
      "Epoch:  172000 | train loss: 4.1313\n",
      "Epoch:  172500 | train loss: 4.1307\n",
      "Epoch:  173000 | train loss: 4.1317\n",
      "Epoch:  173500 | train loss: 4.1306\n",
      "Epoch:  174000 | train loss: 4.1309\n",
      "Epoch:  174500 | train loss: 4.1348\n",
      "Epoch:  175000 | train loss: 4.1312\n",
      "Epoch:  175500 | train loss: 4.1320\n",
      "Epoch:  176000 | train loss: 4.1318\n",
      "Epoch:  176500 | train loss: 4.1331\n",
      "Epoch:  177000 | train loss: 4.1306\n",
      "Epoch:  177500 | train loss: 4.1305\n",
      "Epoch:  178000 | train loss: 4.1305\n",
      "Epoch:  178500 | train loss: 4.1307\n",
      "Epoch:  179000 | train loss: 4.1334\n",
      "Epoch:  179500 | train loss: 4.1353\n",
      "Epoch:  180000 | train loss: 4.1304\n",
      "Epoch:  180500 | train loss: 4.1305\n",
      "Epoch:  181000 | train loss: 4.1304\n",
      "Epoch:  181500 | train loss: 4.1397\n",
      "Epoch:  182000 | train loss: 4.1366\n",
      "Epoch:  182500 | train loss: 4.1303\n",
      "Epoch:  183000 | train loss: 4.1321\n",
      "Epoch:  183500 | train loss: 4.1326\n",
      "Epoch:  184000 | train loss: 4.1312\n",
      "Epoch:  184500 | train loss: 4.1330\n",
      "Epoch:  185000 | train loss: 4.1327\n",
      "Epoch:  185500 | train loss: 4.1341\n",
      "Epoch:  186000 | train loss: 4.1300\n",
      "Epoch:  186500 | train loss: 4.1306\n",
      "Epoch:  187000 | train loss: 4.1309\n",
      "Epoch:  187500 | train loss: 4.1339\n",
      "Epoch:  188000 | train loss: 4.1511\n",
      "Epoch:  188500 | train loss: 4.1306\n",
      "Epoch:  189000 | train loss: 4.1324\n",
      "Epoch:  189500 | train loss: 4.1298\n",
      "Epoch:  190000 | train loss: 4.1355\n",
      "Epoch:  190500 | train loss: 4.1298\n",
      "Epoch:  191000 | train loss: 4.1302\n",
      "Epoch:  191500 | train loss: 4.1297\n",
      "Epoch:  192000 | train loss: 4.1307\n",
      "Epoch:  192500 | train loss: 4.1345\n",
      "Epoch:  193000 | train loss: 4.1303\n",
      "Epoch:  193500 | train loss: 4.1370\n",
      "Epoch:  194000 | train loss: 4.1299\n",
      "Epoch:  194500 | train loss: 4.1296\n",
      "Epoch:  195000 | train loss: 4.1315\n",
      "Epoch:  195500 | train loss: 4.1299\n",
      "Epoch:  196000 | train loss: 4.1295\n",
      "Epoch:  196500 | train loss: 4.1304\n",
      "Epoch:  197000 | train loss: 4.1314\n",
      "Epoch:  197500 | train loss: 4.1322\n",
      "Epoch:  198000 | train loss: 4.1294\n",
      "Epoch:  198500 | train loss: 4.1321\n",
      "Epoch:  199000 | train loss: 4.1296\n",
      "Epoch:  199500 | train loss: 4.1294\n"
     ]
    }
   ],
   "source": [
    "############### Training #####################\n",
    "for epoch in range(EPOCH):    \n",
    "    optimizer.zero_grad() \n",
    "    output = odernn(inputs,numData)\n",
    "    ##forward(self, X0,steps):\n",
    "    ## inputs：X0\n",
    "    ## numData: steps\n",
    "    loss = criterion(output,datas)\n",
    "    loss.backward()                 # backpropagation, compute gradients\n",
    "    optimizer.step()                # apply gradients\n",
    "    if(epoch%500==0):\n",
    "        print('Epoch: ', epoch, '| train loss: %.4f' % loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.6061,  0.7783, -0.6002,  ..., -0.4806, -0.1160, -0.1285],\n",
       "        [ 0.6069,  0.7795, -0.5993,  ..., -0.4815, -0.1167, -0.1285],\n",
       "        [ 0.6060,  0.7788, -0.5996,  ..., -0.4817, -0.1159, -0.1284],\n",
       "        ...,\n",
       "        [ 0.6050,  0.7787, -0.5999,  ..., -0.4816, -0.1157, -0.1293],\n",
       "        [ 0.6070,  0.7796, -0.5996,  ..., -0.4817, -0.1164, -0.1288],\n",
       "        [ 0.6055,  0.7798, -0.6003,  ..., -0.4816, -0.1165, -0.1298]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odernn.cell.Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2857, -0.4098,  0.3796,  ..., -0.1491, -0.0300,  0.0945],\n",
       "        [-0.2867, -0.4096,  0.3797,  ..., -0.1496, -0.0308,  0.0938],\n",
       "        [-0.2868, -0.4098,  0.3801,  ..., -0.1504, -0.0293,  0.0930],\n",
       "        ...,\n",
       "        [-0.2858, -0.4094,  0.3794,  ..., -0.1494, -0.0300,  0.0942],\n",
       "        [-0.2865, -0.4111,  0.3804,  ..., -0.1485, -0.0293,  0.0936],\n",
       "        [-0.2860, -0.4106,  0.3799,  ..., -0.1504, -0.0295,  0.0933]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odernn.cell.Wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91672 ,  0.17323 ,  0.35824 , ..., -0.20776 , -0.1136  ,\n",
       "         0.29931 ],\n",
       "       [ 0.17323 ,  0.027752, -0.055472, ...,  0.68012 ,  0.62477 ,\n",
       "        -0.66241 ],\n",
       "       [ 0.35824 , -0.055472,  0.86062 , ..., -0.72143 , -0.29844 ,\n",
       "         0.57733 ],\n",
       "       ...,\n",
       "       [-0.20776 ,  0.68012 , -0.72143 , ...,  0.49702 ,  0.21797 ,\n",
       "        -0.39859 ],\n",
       "       [-0.1136  ,  0.62477 , -0.29844 , ...,  0.21797 ,  0.54597 ,\n",
       "        -0.33097 ],\n",
       "       [ 0.29931 , -0.66241 ,  0.57733 , ..., -0.39859 , -0.33097 ,\n",
       "         0.79015 ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
